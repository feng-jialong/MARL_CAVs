[MODEL_CONFIG]
; SGD要求样本iid，需要从memory中随机采样，不好用，故用adam
; adam or rmsprop
OPTIMIZER_TYPE = adam
; for the RMSprop optimizer 未使用
; for the Adam optimizer
eps = 1e-6
; 奖励折扣因子
reward_gamma = 0.95
; roll out n trajectories
ROLL_OUT_N_EPISODES = 8
; only remember the latest ROLL_OUT_N_STEPS
MEMORY_CAPACITY = 2024
; only use the latest ROLL_OUT_N_STEPS for training A2C
; 从memory中选择BATCH_SIZE个样本
BATCH_SIZE = 2024
; seeds for pytorch, 0, 2000, 2021
torch_seed = 0
; 目标网络更新频率
TARGET_UPDATE_STEPS = 32
; 目标网络软更新参数，(1-tau)*target+tau*main带入相应的参数
TARGET_TAU = 1.0
actor_hidden_size = 32
critic_hidden_size = 64
; regionalR or global_R
reward_type = regional_R
actor_lr = 5e-4
critic_lr = 5e-4
; 对奖励进行缩放，避免loss太大导致梯度爆炸
; 只在训练时scale，可视化reward时不用
reward_scale = 1000.
; clipping parameter epsilon, 设为0.0即关闭
clip_param = 0.2
; 抽取的batch数
reuse_times = 1
; 每个batch进行更新的次数
epoch_num = 10
use_cuda = True
use_gae = True
; 文献给出的参考数值
gae_lambda = 0.95
use_advantage_norm = False
use_advantage_clip = False
use_value_norm = False
use_value_loss_clip = False
use_value_clip = False
use_policy_loss_clip =False
; 可能阻碍了tensorboard的断点续训,原因?
use_observation_norm = True
use_observation_clip = False
; ReLU或tanh
activation_type = tanh
use_orthogonal_init = False
use_layer_norm = False
; MSE or huber
; 作为字符串读取的配置参数不要和注释放在同一行
critic_loss = MSE
use_ppo_clip = True
use_kl_penalty = False
kl_beta = 0.1
; 梯度截断
MAX_GRAD_NORM = 5.
; not implement yet
use_adaptive_kl_penalty = False
; 需要是ROLL_OUT_N_EPISODES的倍数
scene_frequency = 640
; 是否使用信息熵损失函数
use_entropy_loss = True
; 信息熵损失函数系数
entropy_loss_beta = 0.005
; critic target的估计方法,TD or MC
critic_target_type = TD
use_param_share = False
obs_dim = 7
actor_shared_size = 8
critic_shared_size = 8

[TRAIN_CONFIG]
MAX_EPISODES = 100
EPISODES_BEFORE_TRAIN = 0
; 必须是ROLL_OUT_N_EPIDODES的倍数
EVAL_INTERVAL = 128
EVAL_EPISODES = 0
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575

[EVAL_CONFIG]
EVAL_MAX_EPISODES = 20

[ENV_CONFIG]
; seed for the environment
seed = 0
; 每秒仿真步步数
simulation_frequency = 15
; 每秒决策步步数
policy_frequency = 5
; 仿真时长(仿真秒)
duration = 30
; 极大的碰撞惩罚
COLLISION_REWARD = 1000
; 速度奖励系数
HIGH_SPEED_REWARD = 2
max_reward_speed_range = 28.
min_reward_speed_range = 0.
; 聚群奖励系数
FLOCKING_REWARD = 1
FLOCKING_UB = 200.
FLOCKING_LB = 20.
; 车距惩罚系数
HEADWAY_COST = 2
; 网联自动下的最小跟车时距，需要仔细考量
HEADWAY_TIME = 1
; 换道惩罚，惩罚左右摇摆加快策略稳定
; 换道惩罚系数
LANE_CHANGE_COST = 0.1

lanes_number = 3
CAV_number = 3
; 预设好训练场景，这个设置用不上了
HDV_number = 2 
; 见abstract的reset方法
action_masking = True

[OTHER_CONFIG]
; experiment name
name = test