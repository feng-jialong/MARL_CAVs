[MODEL_CONFIG]
; SGD要求样本iid，需要从memory中随机采样，不好用，故用adam
; adam or rmsprop
OPTIMIZER_TYPE = adam
; for the RMSprop optimizer 未使用
; for the Adam optimizer
eps = 1e-6
; 奖励折扣因子
reward_gamma = 0.95
; roll out n trajectories
ROLL_OUT_N_EPISODES = 8
; only remember the latest ROLL_OUT_N_STEPS
MEMORY_CAPACITY = 4096
; only use the latest ROLL_OUT_N_STEPS for training A2C
; 从memory中选择BATCH_SIZE个样本
BATCH_SIZE = 4096
; seeds for pytorch, 0, 2000, 2021
torch_seed = 0
; 目标网络更新频率
TARGET_UPDATE_STEPS = 32
; 目标网络软更新参数，(1-tau)*target+tau*main带入相应的参数
TARGET_TAU = 1.0
actor_hidden_size = 32
critic_hidden_size = 32
; regionalR or global_R
reward_type = global_R
actor_lr = 5e-4
critic_lr = 5e-4
; 对奖励进行缩放，避免loss太大导致梯度爆炸
; 只在训练时scale，可视化reward时不用
reward_scale = 1000.
; clipping parameter epsilon, 设为0.0即关闭
clip_param = 0.2
; 抽取的batch数
reuse_times = 1
; 每个batch进行更新的次数
epoch_num = 10
use_cuda = True
use_gae = True
; 文献给出的参考数值
gae_lambda = 0.95
use_advantage_norm = False
use_advantage_clip = False
use_value_norm = False
use_value_loss_clip = False
use_value_clip = False
use_policy_loss_clip =False
; 可能阻碍了tensorboard的断点续训,原因?
use_observation_norm = True
use_observation_clip = False
; ReLU或tanh
activation_type = ReLU
use_orthogonal_init = False
use_layer_norm = False
; MSE or huber
; 作为字符串读取的配置参数不要和注释放在同一行
critic_loss = MSE
use_ppo_clip = True
use_kl_penalty = False
kl_beta = 0.1
; 梯度截断
MAX_GRAD_NORM = 5.
; not implement yet
use_adaptive_kl_penalty = False
; 需要是ROLL_OUT_N_EPISODES的倍数
scene_frequency = 32
; 是否使用信息熵损失函数
use_entropy_loss = False
; 信息熵损失函数系数
entropy_loss_beta = 0.005
; critic target的估计方法,TD or MC
critic_target_type = TD
use_param_share = True
obs_dim = 6
actor_shared_size = 8
critic_shared_size = 8
use_centralized_critic = False
use_rnn = False
collision_replay_times = 1
max_obs = 10

[TRAIN_CONFIG]
MAX_EPISODES = 200
EPISODES_BEFORE_TRAIN = 0
; 必须是ROLL_OUT_N_EPIDODES的倍数
EVAL_INTERVAL = 8
EVAL_EPISODES = 1
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575

[EVAL_CONFIG]
EVAL_MAX_EPISODES = 20

[ENV_CONFIG]
; seed for the environment
seed = 0
; 每秒仿真步步数
simulation_frequency = 15
; 每秒决策步步数
policy_frequency = 5
; 仿真时长(仿真秒)
duration = 20
; 极大的碰撞惩罚
COLLISION_REWARD = 1000
; 速度奖励系数
HIGH_SPEED_REWARD = 5
; 聚群奖励系数
FLOCKING_REWARD = 10
; 驾驶舒适度奖励系数
COMFORT_REWARD = 1
; 驾驶安全性奖励
SAFE_DECISION_REWARD = 5
; 车道数
lanes_number = 3
CAV_number = 3
DESIRE_SPEED = 25.0

[ENV_EVAL_CONFIG]
CAV_number = 3
lanes_number = 3

[OTHER_CONFIG]
; experiment name
name = exp